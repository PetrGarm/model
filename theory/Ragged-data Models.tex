\documentclass[8pt, a4paper, twocolumn, landscape]{extarticle}

\usepackage{geometry}
\usepackage{amsmath}
\geometry{
left=1.5cm,
right=1.5cm,
top=2cm,
bottom=2cm,
headsep=3ex,
footskip=4ex
}

\usepackage{icomma} 
\usepackage{tgadventor}

\title{Ragged-data Models}
\author{kassandra}

\renewcommand{\baselinestretch}{1.5}


\begin{document}
	
	\maketitle
	
	\section{Massimiliano Marcellino, Christian Schumacher: "Factor-MIDAS for Now- And Forecasting with Ragged-Edge Data: A Model Comparison for German GDP" (2008)}

	
	They focus on a quarterly GDP growth ($y_{t_q}$), where $t_q$ is a quarter time index ($t_q = 1, 2, 3, \dots, T_q$). Also they consider monthly frequency ($y_{t_m}$): $y_{t_m} = y_{t_q} \forall t_m = 3t_q \Rightarrow t_m = 3, 6, 9, \dots, T_m, T_m = 3T_q$.
	
	Their aim is to predict GDP growth for $h_q$ quarters ($h_m = 3h_q$ months) ahead. They do it by:
	\begin{enumerate}
		\item Taking the factors out of data:
		\[
		X_{t_m} = \Lambda F_{t_m} + \xi_{t_m},
		\]
		where 
		
		$X_{t_m}$ is an information set, $N$-dimensional vector containing a large set of stationary monthly indicators,
		
		$\Lambda$ is a loadings matrix ($N\times r$),
		
		$F_{t_m}$ is a $r$-dimensional factor vector,
		
		$\xi_{t_m}$ is a component of $X_{t_m}$ not explained by factors. 
		
		\item Applying the MIDAS model to these factors.
	\end{enumerate}
	They present three ways to take the factors out.
	
	\subsection{Way 1: Vertical Realignment of Data and Dynamic Principal Components Factors (Based on Altissimo et al. (2006))}
	
	The idea is to realign each time series in a sample to obtain a balanced dataset.
	
	Assume variable $i$ has a publication lag of $K_i$ months. Then for period $T_m$ the last available observation will be $T_m - K_i$. The realignment procedure is the following one:
	\[
	\tilde{x}_{i, T_m} = x_{i, T_m - K_i}
	\]
	
	for $t_m = K_i + 1, \dots, T_m$.
	
	Applying this procedure to all the series and harmonizing the beginning of the sample, we get a balanced dataset $\tilde{X}_{t_m}$ for $t_m = \max (\{k_i\}^N_{i=1}) + 1, \dots, T_m$.
	
	Then dynamic PCA is applied to estimate factors.
	
	Positive: it is simple. Negative: statistics is published at different moments and factors may be reassessed more often than the model frequency is. These lead to realignment changes the correlation structure of data (however, there is an opinion stated that dynamic PCA is able to overcome this problem).
	
	\subsection{Way 2: Principal Components Factors and the EM algorithm (Basrd on Stock and Watson (2002))}
	
	Assume variable $i$ from $X_{t_m}$ is presented with a full vector $X_i = (x_{i, 1} \dots x_{i, T_m})$. If there is a ragged-edge problem, let a vector $X_i^{obs}$ contain all the available observations ($X_i^{obs} \subset X_i$):
	\[
	X_i^{obs} = A_i X_i,
	\]
	where $A_i$ is a matrix which can tackle missing values or frequencies. If there is no missing data, $A_i$ is an identity matrix. If there is missing data in the end of the sample (due to the rag), the corresponding row is deleted from $A_i$. Then the EM algorithm is applied:
	\begin{enumerate}
		\item Make a naive prediction of $\hat{X}_i^{(0)} \forall i \Rightarrow$ get a balanced dataset $\hat{X}^{(0)}$. Standard PCA provides $\hat{F}^{(0)}$ and $\hat{\Lambda}^{(0)}$.
		\item E-step. Update:
		\[
		\hat{X_i}^{(j)} = \hat{F}^{(j-1)} \hat{\Lambda}_i^{(j-1)} + A_i'(A_i'A_i)^{-1}(X_i^{obs} - A_i \hat{F}^{(j-1)} \hat{\Lambda}_i^{(j-1)}).
		\]
		\item M-Step. Repeat E-step for all $i$, getting balanced dataset. Reestimate $\hat{F}^{(j)}$ and $\hat{\Lambda}^{(j)}$ using PCA, repeat E-Step until convergence.
	\end{enumerate}

	\subsection{Way 3: Estimation of a Large Parametric Factor Model in The State-Space Form (Based on Doz et al. (2006))}
	
	An explicit dynamic VAR structure is assumed to hold for the factors. The full state-space model is as following:
	\[
	X_{t_m} = \Lambda F_{t_m}  + \xi_{t_m}
	\]
	\[
	\Psi(L_m) F_{t_m} = B \eta_{t_m}
	\]
	
	The second row is the VAR of the factors, where:
	\smallskip
	
	 $\Psi(L_m) = \sum_{i=1}^{p} \Psi_i L_m^i$ is a lag polynomial,
	 \smallskip
	 
	 $L_m$ is a monthly lag operator, $L_m x_{t_m} = x_{t_m - 1}$,
	 \smallskip
	 
	 $\eta_{t_m}$ is a $q$-dimensional vector containing orthogonal dynamic shocks that drive $r$ factors. 
	 \smallskip
	 
	 $B$ is a $(r \times q)$ matrix.
	 
	 If the dimension of $X_{t_m}$ is small, the model is estimated using ML. Otherwise, using quasi-ML:
	 
	 \begin{enumerate}
	 	\item Estimate $\hat{F_{t_m}}$ using PCA as an initial estimate.
	 	
	 	\item Estimate $\hat{\Lambda}$ by regressing $X_{t_m}$ on the estimated factors $\hat{F_{t_m}}$. Estimate covariance $\hat{\xi}_{t_m} = X_{t_m} - \hat{\Lambda}\hat{F}_{t_m}$ (this covariance is denoted as $\hat{\Sigma}_{\xi}$).
	 	
	 	\item Estimate factor VAR($p$) on the factors $\hat{F}_{t_m}$ which gives $\hat{\Psi}(L)$ and the residual covariance of $\hat{c}_{t_m} = \hat{\Psi} (L_m) \hat{F}_{t_m}$. This covariance is denoted as $\hat{\Sigma}_{c}$).
	 	
	 	\item To obtain $B$ with given $q$, perform eigenvalue decomposition of $\hat{\Sigma}_{c}$). Let $M$ be an $(r \times q)$ matrix containing eigenvectors corresponding to $q$ biggest eigenvalues, and $P$ be a $(q \times q)$ matrix, containing these biggest eigenvalues on the main diagonal and zeros otherwise. Then $\hat{B} = \frac{M}{\sqrt{P}}$.
	 	
	 	\item The coefficients and auxiliary parameters are fully specified numerically. The model is cast into space-state form. To estimate factors use Kalman filter or smoother.
	 \end{enumerate}
	 
	 \subsection{Factor-MIDAS: Predict Monthly GDP Growth Using Estimated Factors}
	 
	 MIDAS stands for "mixed-data sampling".
	 
	 \subsubsection{Basic Factor-MIDAS}
	 
	 General model:
	 \[
	 y_{t_q + h_q} = y_{t_m + h_m} = \beta_0 + \sum_{i = 1}^{r} \beta_{1, i} b_i (L_m, \theta_i) \hat{f}_{i, t_m}^{(3)} + \varepsilon_{t_m + h_m}.
	 \]
	 
	 Considering a case with $r = 1$, i.e. only one factor $\hat{f}_{t_m}$ is used, the forecast for $h_q$ quarters ($h_m = 3h_q$ months) is as following:
	 \[
	 y_{t_q + h_q} = y_{t_m + h_m} = \beta_0 + \beta_{1} b (L_m, \theta) \hat{f}_{t_m}^{(3)} + \varepsilon_{t_m + h_m},
	 \]
	 
	 where
	 \smallskip
	 
	 $b(L_m, \theta) = \sum_{k=0}^{K} z(k, \theta) L_m^k$
	 \smallskip
	 
	 $z(k, \theta) = \frac{\exp{(\theta_1k + \theta_2k^2)}}{\sum_{k=0}^{K}\exp{(\theta_1k + \theta_2k^2)}}$
	 \smallskip
	 
	 $\hat{f}_{t_m}^{(3)} = \hat{f}_{t_m} \forall t_m = \dots, T_{m-6}, T_{m-3}, T_m$. Lags are treated accordingly.
	 \medskip
	 
	 The model can be estimated using nonlinear least squares in a regression in a regression of $y_{t_m}$ onto $\hat{f}_{t_m - k}^{(3)}$, which gives coefficients $\hat{\theta}_1$, $\hat{\theta}_2$, $\hat{\beta}_0$, $\hat{\beta}_1$.
	 \medskip
	 
	 The forecast is as following:
	 \[
	 y_{T_m + h_m | T_m} = \hat{\beta}_0 + \hat{\beta}_1 b (L_m, \hat{\theta}) \hat{f}_{T_m}.
	 \]
	
	\subsubsection{Smoothed MIDAS (Based on Altissimo et al. (2006))}
	
	They consider New Eurocoin index and present the following projection:
	\[
	y_{T_m + h_m | T_m} = \hat{\mu} + G\hat{F}_{T_m}
	\]
	\[
	G = \tilde{\Sigma}_{yF}(h_m) \times \hat{\Sigma}_F^{-1},
	\]
	
	where
	\smallskip
	
	$G$ is the projection coefficient matrix.
	\smallskip
	
	$\tilde{\Sigma}_{yF}(h_m)$ is the cross-covariance with $h_m$ monthly lags between the GDP growth and factors. 
	\smallskip
	
	$\hat{\Sigma}_F$ is the estimated sample covariance of the factors.
	\medskip
	
	The smooth component is within $\tilde{\Sigma}_{yF}(h_m)$. Assume the factors and the GDP growth are demeaned. Then let the covariance between $\hat{F}_{t_m-k}$ and $y_{t_m}$ be estimated by
	
	\[
	\hat{\Sigma}_{yF}(k) = \frac{1}{T^* - 1}\sum_{t_m = M + 1}^{T_m} y_{t_m} \hat{F}_{t_m - k}^{(3)'},
	\]
	
	where $T^*$ = floor$[(T_m - (M+1))/3]$ is the number of observations available to compute cross-covariance for $k = -M \dots M$ and $M \ge 3h_q = h_m$.
	
	Then the estimation of cross-spectral matrix is as following:
	\[
	\hat{S}_{yF}(w_j) = \sum_{k = -M}^{M} \left( 1 - \frac{|k|}{M + 1} \right) \hat{\Sigma}_{yF}(k)e^{iw_jk}
	\]
	
	at frequencies $w_j = \frac{2\pi j}{2H}$ for $i = -H, \dots, H$ (Bartlett lag window). By reverse Fourier transform they obtain:
	\[
	\tilde{\Sigma}_{yF}(k) = \frac{1}{2H + 1} \sum_{j = -H}^{H} \alpha (w_j) \hat{S}_{yF}(w_j)e^{iw_jk}
	\]
	
	where $\alpha(w_j)$ is a frequency-response function. In Eurocoin example,
	\[
	\alpha = \begin{cases}
	1, & \forall |w_j| < \frac{\pi}{6} \\
	0, & \textnormal{otherwise}.
	\end{cases}
	\]
	
	\subsubsection{The Unrestricted MIDAS (Based on Marcellino and Schumacher (2007))}
	
	They consider an unrestricted lag order model:
	\[
	y_{T_m + h_m} = \beta_0 + D(L_m)F_{t_m}^{(3)} + \varepsilon_{t_m + h_m},
	\]
	where $D(L_m) = \sum_{k = 0}^{K}D_kL_m^k$ is an unrestricted lag polynomial of frequency $k$. $D(L_m)$ and $\beta_0$ are estimated by OLS.
	
	
	\section{Kees E. Bouwman, Jan P.A.M. Jacobs: "Forecasting with real-time macroeconomic data: the ragged-edge problem and revisions" (2005)}
	
	\subsection{Assumptions}
	
	They claim that, considering linear time series models, ragged-data problem can be solved by the Kalman filter (Harvey (1989) and Hamilton (1994, Chapter 13)). They assume the following:
	\begin{enumerate}
		\item The maximum publication lag is equal to one month.
		\item $x_1(t), t \in N$ is the vector of final values for period $t$ of the variables released without a publication lag.
		\item $x_2(t), t \in N$ is the vector of final values for period $t$ of the variables released with an one-month publication lag.
		\item The data becomes final by the end of the 5-month period since the first publication, so that there is a maximum of 5 data releases and 4 data revisions (updates) for a period. $x_k(i, t), k = 1, 2$, $ i = 1, \dots, 5$ is the $i$-th release of the value $x_k$ for a period $t$. The release period of $x_k(i, t)$ is denoted by $\tau_k(i, t)$:
		\[
		\tau_1(i, t) = t + i,
		\]
		\[
		\tau_2(i, t) = t + i + 1.
		\]
		
		As the 5-th release is final, $x_k(t) \equiv x_k(5, t)$ and $\tau_k(t) \equiv \tau_k(5, t)$. 
		\item A new vintage is released every month. All values in a vintage are given by their latest available release:
		\[
		x_1^T(t) = x(\min(T-t, 5), t) \, \forall t < T,
		\]
		\[
		x_2^T(t) = x(\min(T-t-1, 5), t) \, \forall t < T-1,
		\]
		where $T$ is the vintage date.
		\item All the information available in period $T$ is represented by the information
		set
		\[
		\Omega^T = \{ x_k(i, t) : \tau_k(i, t) \le T\},
		\]
		where $	t = 1, \dots, T-1$.
		
		They say that many forecast evaluations consider forecasts based on final data, which makes the information set to be the following:
		\[
		\tilde{\Omega}^T = \{ x_k(t) : \tau_k(1, t) \le T\}.
		\]
	
	\end{enumerate}

\subsection{Modelling final data: TCB}

They claim that to predict missing observations one has to specify the dynamics of the final data itself. To handle it they present the TCB procedure, which ignores data revisions and employs univariate AR(2) models on the levels for the imputation:

\begin{align*}
	\hat{x}_1(t | \Omega_T)  &= x_1^T(t), \hspace{3.86cm} t \le T-1, \\
	\hat{x}_2(t | \Omega_T)  &= \begin{cases}
	x_2^T(t), & t < T-1, \\
	b + A_1 x_2^T(t-1) + A_2 x_2^T(t-2), & t = T-1,
	\end{cases}
\end{align*}
where $A_1$ and $A_2$ are diagonal parameter matrices and $b$ is a parameter vector arising from modeling the components of $x_2$ separately by AR(2) models with a constant included. The parameter estimates can be obtained from historical data.

\subsection{Modelling final data: Transformed Indicators}
An alternative approach is to model the dynamics of the final data in terms of trans- formed indicators $r_k$ of $x_k$:
\[
r_k(t) = g_k(x_k(\cdot)), \, k = 1, 2.
\]
They assume a $p$-th order linear model:
\[
r(t) = b + A_1r(t-1) + \dots + A_pr(t-p) + \varepsilon(t),
\]
where $r(t) = \left(r_1(t)', r_2(t)'\right)'$ and $\varepsilon(t) \sim GWN(0, \Sigma_{\varepsilon})$.

To put the model into the state-space form, they define a state vector as 
\[
\alpha(t) = \left( r(t)', \dots, r(t-p+1)' \right)'.
\]

Then the state-space form is given by the measurement equation:
\[
r(t) = \begin{bmatrix}
I & 0 & \dots & 0
\end{bmatrix} \alpha(t),
\]
and the transition equation:
\[
\alpha(t+1) = 
\begin{bmatrix}
A_1 & \dots & A_{p-1} & A_p \\
I & \dots & 0 & 0 \\
\vdots & \ddots & \vdots & \vdots \\
0 & \dots & I & 0
\end{bmatrix} \alpha(t) + 
\begin{bmatrix}
I \\
0 \\
\vdots \\
0
\end{bmatrix}b +
\begin{bmatrix}
I \\
0 \\
\vdots \\
0
\end{bmatrix}\varepsilon(t),
\]
where $I$ is the identity matrix. 
They claim that the ragged edge can be smoothed by imputing the delayed observations of $r_2(t)$ with the Kalman filter.

\subsection{Modelling final data: Alternative Models}
They also mention alternative models:
\begin{enumerate}
	\item AR$(p)$: $A_1, \dots, A_p$ and $\Sigma_{\varepsilon}$ are restricted to be diagonal. The parameters are estimated by OLS for the individual AR(2) models. $\Sigma_{\varepsilon}$ is formed by the variances of the residuals of the individual equations.
	\item SUR$(p)$: $A_1, \dots, A_p$ are restricted to be diagonal but no restrictions are imposed on $\Sigma_{\varepsilon}$. The parameters are estimated by the Seemingly Unrelated Regression procedure.
	\item VAR$(p)$:  $A_1, \dots, A_p$ and $\Sigma_{\varepsilon}$ are all free. Parameters are estimated by OLS for the individual equations. $\Sigma_{\varepsilon}$ is estimated from the residuals.
\end{enumerate}

\subsection{Modelling final data: Adjusting for Data Revisions}
They say that one approach to consider is to ignore revision errors at all and make a naive prediction based on the last available data vintage:
\[
\hat{x}_k(t | \Omega^T) = x_k^T(t) \text{ for } t \le T - k.
\]

Another approach takes data revisions into account. They claim that given the observed provisional values a typical model yields the conditional distribution of the unobserved final values. From these densities one can derive the Minimum Mean Squared Error predictions of the final values:
\[
\hat{x}_k(t | \Omega^T) = E(x_k(t) | \Omega^T) \text{ for } t \le T - 1.
\]

Next, they consider the extension of the state-space model, which will take into account data revisions. The measurement error model they consider assumes that preliminary values are final values contaminated with an additive measurement error, which follows a GWN process. Under this assumption, the final state-space model looks as following: the transition equation from above remains unchanged, whereas the measurement equation becomes:
\[
y(t) = 
\begin{pmatrix}
\i \, \otimes \, 
\begin{bmatrix}
I & 0 & \dots & 0
\end{bmatrix}
\end{pmatrix} \alpha(t) + 
\begin{pmatrix}
\begin{bmatrix}
1 & 1 & 1 & 1 \\
0 & 1 & 1 & 1 \\
0 & 0 & 1 & 1 \\
0 & 0 & 0 & 1 \\
0 & 0 & 0 & 0
\end{bmatrix} \otimes I
\end{pmatrix} v(t),
\]
where

$y(t) = \left(r(1,t)', \dots r(5,t)' \right)'$,

$\i$ is the unit vector of length 5,

$v(t)$ is the vector of revision errors:
\[
v(t) = \begin{pmatrix}
u(1, t) \\
\vdots \\
u(4, t)
\end{pmatrix},
\]
\[
u(i, t) = r(i, t) - r(i + 1, t), \text{ for } i = 1, \dots, 4.
\]
Moreover, if $\eta_j(t) = (u_j(1, t), \dots, u_j(4, t))'$, the revision model assumes:
\[
\eta_j \sim GWN(0, \Sigma_{\eta j}),
\]
\[
E(\eta_j(t), \eta_k(t)') = 0 \text{ for } j \ne k,
\]
\[
E(\eta_j(t), \varepsilon(t)') = 0,
\]
which yields the following variance-covariance matrix of the revision errors:
\[
E(v(t),v(t)') = K_{10, 4} \begin{pmatrix}
\Sigma_{\eta, 1} & \, & 0 \\
\, & \ddots & \, \\
0 & \, & \Sigma_{\eta, 10}
\end{pmatrix} K_{4, 10},
\]
where $K_{m, n}$ is the commutation matrix: $K_{m,n} \text{ vec } A = \text{ vec } A'$ for an arbitrary $(m \times n)$-matrix $A$.

Returning to the model, again, the Kalman filter can be used to impute the delayed observations $r_2(t)$.

\end{document}